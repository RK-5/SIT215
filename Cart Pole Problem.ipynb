{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tutorial has been used to implement Random Policy:\n",
    "\n",
    "http://kvfrans.com/simple-algoritms-for-solving-cartpole/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, parameters):\n",
    "    #reset environment\n",
    "    observation = env.reset()\n",
    "    #total rewards gathered (get 1 for each timestep pole has not been dropped)\n",
    "    totalreward = 0\n",
    "    #run for 200 timesteps\n",
    "    for _ in range(200):\n",
    "        #env.render()\n",
    "        #multiply matrix of parameters by obervations\n",
    "        #if the total is less than 0 set action to be move left, otherwise move right\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        #execute action\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        #give 1 reward\n",
    "        totalreward += reward\n",
    "        #if pole fell over, break the loop. Episode ends.\n",
    "        if done:\n",
    "            break\n",
    "    #env.close()\n",
    "    #return how may timesteps the pole way able to be balanced for an episode\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(submit):\n",
    "    #load the CartPole environment\n",
    "    env = gym.make('CartPole-v0')\n",
    "           \n",
    "    #how many episodes it took for the pole to be balanced for 200 timesteps\n",
    "    counter = 0\n",
    "    #best starting weights/observations\n",
    "    bestparams = None\n",
    "    #best reward achieved from an episode\n",
    "    bestreward = 0\n",
    "    \n",
    "    #run 10000 episodes\n",
    "    for _ in range(10000):\n",
    "        counter += 1\n",
    "        #generate random weights/observations\n",
    "        parameters = np.random.rand(4) * 2 - 1\n",
    "        #get the total number of rewards for that episode, with random weights/observations\n",
    "        reward = run_episode(env,parameters)\n",
    "        #if the rewards from current episode were better than last best episode\n",
    "        if reward > bestreward:\n",
    "            #set the best episode to be the current one \n",
    "            bestreward = reward\n",
    "            #set best weights/observations to be those used for the current episode\n",
    "            bestparams = parameters\n",
    "            # considered solved if the agent lasts 200 timesteps\n",
    "            if reward == 200:\n",
    "                break\n",
    "\n",
    "    if submit:\n",
    "        for _ in range(100):\n",
    "            env.render()\n",
    "            run_episode(env,bestparams)\n",
    "        env.close()\n",
    "\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(submit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graphs\n",
    "results = []\n",
    "for _ in range(1000):\n",
    "    results.append(train(submit=False))\n",
    "\n",
    "plt.hist(results,50,density=1, facecolor='g', alpha=0.75)\n",
    "plt.xlabel('Episodes required to reach 200')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Random Search')\n",
    "plt.show()\n",
    "\n",
    "print(np.sum(results) / 1000.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, parameters):\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    counter = 0\n",
    "    for _ in range(200):\n",
    "        # env.render()\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        counter += 1\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(submit):\n",
    "    env = gym.make('CartPole-v0')\n",
    "    \n",
    "    episodes_per_update = 5\n",
    "    noise_scaling = 0.1\n",
    "    parameters = np.random.rand(4) * 2 - 1\n",
    "    bestreward = 0\n",
    "    counter = 0\n",
    "\n",
    "    for _ in range(2000):\n",
    "        counter += 1\n",
    "        newparams = parameters + (np.random.rand(4) * 2 - 1)*noise_scaling\n",
    "        \n",
    "        reward = run_episode(env,newparams)\n",
    "        \n",
    "        if reward > bestreward:\n",
    "            bestreward = reward\n",
    "            parameters = newparams\n",
    "            if reward == 200:\n",
    "                break\n",
    "\n",
    "    if submit:\n",
    "        for _ in range(100):\n",
    "            env.render()\n",
    "            run_episode(env,bestparams)\n",
    "        env.close()\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graphs\n",
    "results = []\n",
    "for _ in range(1000):\n",
    "    results.append(train(submit=False))\n",
    "\n",
    "plt.hist(results,50,density=1, facecolor='g', alpha=0.75)\n",
    "plt.xlabel('Episodes required to reach 200')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Hill Climbing')\n",
    "plt.show()\n",
    "\n",
    "print(np.sum(results) / 1000.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tutorial has been used to implement Q-Learning:\n",
    "https://medium.com/@flomay/using-q-learning-to-solve-the-cartpole-balancing-problem-c0a7f47d3f9d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleQAgent():\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialise the environment and hyperparameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, buckets=(3, 3, 6, 6), \n",
    "                 num_episodes=500, min_lr=0.1, \n",
    "                 min_epsilon=0.1, discount=1.0, decay=25):\n",
    "        self.buckets = buckets\n",
    "        self.num_episodes = num_episodes\n",
    "        self.min_lr = min_lr\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.discount = discount\n",
    "        self.decay = decay\n",
    "\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        \n",
    "        # Initialise Q_table\n",
    "        self.Q_table = np.zeros(self.buckets + (self.env.action_space.n,))\n",
    "\n",
    "        # set upper and lower bounds for [position, velocity, angle, angular velocity]\n",
    "        self.upper_bounds = [self.env.observation_space.high[0], 0.5, self.env.observation_space.high[2], math.radians(50) / 1.]\n",
    "        self.lower_bounds = [self.env.observation_space.low[0], -0.5, self.env.observation_space.low[2], -math.radians(50) / 1.]\n",
    "        \n",
    "        # used to store timesteps for each episode\n",
    "        self.steps = np.zeros(self.num_episodes)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    discretize_state() is used to discretize the observation values, which are continuious.\n",
    "    This is done by grouping similar values together, so that the Q-table space is reduced and it can be filled easily.\n",
    "    \"\"\"\n",
    "    def discretize_state(self, obs):\n",
    "        discretized = list()\n",
    "        for i in range(len(obs)):\n",
    "            scaling = ((obs[i] + abs(self.lower_bounds[i])) \n",
    "                       / (self.upper_bounds[i] - self.lower_bounds[i]))\n",
    "            new_obs = int(round((self.buckets[i] - 1) * scaling))\n",
    "            new_obs = min(self.buckets[i] - 1, max(0, new_obs))\n",
    "            discretized.append(new_obs)\n",
    "        return tuple(discretized)\n",
    "\n",
    "    \"\"\"\n",
    "    choose_action() is used to choose the next action in a greedy way, \n",
    "    using either a random action or using q-values from q-table for the given state.\n",
    "    \"\"\"\n",
    "    def choose_action(self, state):\n",
    "        if (np.random.random() < self.epsilon):\n",
    "            return self.env.action_space.sample() \n",
    "        else:\n",
    "            return np.argmax(self.Q_table[state])\n",
    "    \"\"\"\n",
    "    get_action() is a variation of the previous function. It will assume that the state passed in\n",
    "    is not discretized, and calculates probabilites for each action.\n",
    "    \"\"\"\n",
    "    def get_action(self, state, e):\n",
    "        obs = self.discretize_state(state)\n",
    "        action_vector = self.Q_table[obs]\n",
    "        epsilon = self.get_epsilon(e)\n",
    "        action_vector = self.normalize(action_vector, epsilon)\n",
    "        return action_vector\n",
    "\n",
    "    \"\"\"\n",
    "    normalize() is used to calculate the probability of each action being chosen for the state passed in.\n",
    "    \"\"\"\n",
    "    def normalize(self, action_vector, epsilon):        \n",
    "        total = sum(action_vector)\n",
    "        new_vector = (1-epsilon)*action_vector/(total)\n",
    "        new_vector += epsilon/2.0\n",
    "        return new_vector\n",
    "\n",
    "    \"\"\"\n",
    "    update_q() is used to calculate the Q-value for a given state and action, and then update the Q-table.\n",
    "    \"\"\"\n",
    "    def update_q(self, state, action, reward, new_state):\n",
    "        self.Q_table[state][action] += (self.learning_rate * (reward + self.discount * np.max(self.Q_table[new_state]) - self.Q_table[state][action]))\n",
    "\n",
    "    \"\"\"\n",
    "    get_epsilon() is used to get the epsilon value, which decays or declines as episodes are run.\n",
    "    \"\"\"\n",
    "    def get_epsilon(self, t):\n",
    "        # Ensures that there's almost at least a min_epsilon chance of randomly exploring\n",
    "        return max(self.min_epsilon, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
    "    \n",
    "    \"\"\"\n",
    "    get_learning_rate() is used to get the value for the learning rate. Which declines as episodes are run.\n",
    "    \"\"\"\n",
    "    def get_learning_rate(self, t):\n",
    "        \n",
    "        # Learning rate also declines as we add more episodes\n",
    "        return max(self.min_lr, min(1., 1. - math.log10((t + 1) / self.decay)))\n",
    "\n",
    "    \"\"\"\n",
    "    Used to train the agent using Q-Learning using the greedy method. It gets trained by default for 500 episodes. \n",
    "    \"\"\"\n",
    "    def train(self):\n",
    "        # Looping for each episode\n",
    "        for e in range(self.num_episodes):\n",
    "            # Initializes the state\n",
    "            current_state = self.discretize_state(self.env.reset())\n",
    "\n",
    "            self.learning_rate = self.get_learning_rate(e)\n",
    "            self.epsilon = self.get_epsilon(e)\n",
    "            done = False\n",
    "            \n",
    "            # Looping for each step\n",
    "            while not done:\n",
    "                self.steps[e] += 1\n",
    "                # Choose A from S\n",
    "                action = self.choose_action(current_state)\n",
    "                # Take action\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize_state(obs)\n",
    "                # Update Q(S,A)\n",
    "                self.update_q(current_state, action, reward, new_state)\n",
    "                current_state = new_state\n",
    "                \n",
    "                # We break out of the loop when done is False which is\n",
    "                # a terminal state.\n",
    "        print('Finished training!')\n",
    "    \n",
    "    \"\"\"\n",
    "    Used to create a line plot to show how long the agent is able to balance the pole at each episode.\n",
    "    \"\"\"\n",
    "    def plot_learning(self):\n",
    "        sns.lineplot(range(len(self.steps)),self.steps)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Steps\")\n",
    "        plt.title(\"CartPole-Problem\")\n",
    "        plt.show()\n",
    "        t = 0\n",
    "        for i in range(self.num_episodes):\n",
    "            if self.steps[i] == 200:\n",
    "                t+=1\n",
    "        #how many times the agent was able to balance the pole for 200 timesteps.\n",
    "        print(t, \"episodes were successfully completed.\")\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    Used after Q-Learning training to visualise how well the agent is able to perform now.\n",
    "    \"\"\"\n",
    "    def run(self):\n",
    "        \"\"\"Runs an episode while displaying the cartpole environment.\"\"\"\n",
    "        self.env = gym.wrappers.Monitor(self.env,'cartpole')\n",
    "        t = 0\n",
    "        done = False\n",
    "        current_state = self.discretize_state(self.env.reset())\n",
    "        while not done:\n",
    "                self.env.render()\n",
    "                t = t+1\n",
    "                action = self.choose_action(current_state)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                new_state = self.discretize_state(obs)\n",
    "                current_state = new_state\n",
    "            \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Driver code\n",
    "\"\"\"\n",
    "def load_q_learning():\n",
    "    agent = CartPoleQAgent()\n",
    "    agent.train()\n",
    "    agent.plot_learning()\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load_q_learning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
