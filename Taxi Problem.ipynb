{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install OpenAI gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the game environment and render what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core gym interface is `env`.\n",
    "\n",
    "`env.reset`: Resets the environment and returns a random initial state.\n",
    "\n",
    "`env.step(action)`: Step the environment by one timestep.\n",
    "\n",
    "`env.render`: Renders one frame of the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the state, so that the taxi is at row 3, column 1 (3,1). The passenger is at location 2, and the destination is at location 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the reward table, where the rows are the actions (south, north, east, west, pickup, dropoff). The columns are (probability, nextstate, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will try to brute-force our way to solving the problem withould Reinforcement Learning. Because we have a default rewards table for each state `P` we use that to navigate the taxi.\n",
    "\n",
    "The infinite loop will run until one passenger reaches a destination (one episode). Simply, when the agent gets awarded with 20 points. \n",
    "\n",
    "`env.action_space.sample()`: automatically selects one random action from set of all possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "#print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result is not great. The agent takes many steps and makes a lot of dropoffs at incorrect locations to just deliver one passenger to the correct destination. This is due to the agent not 'learning' from past experiences, meaning that it doesn't matter how often we run the simulation, the agent will never be optimised. The agent has no memory of which action is best for each state, which is where Reinforcement Learning can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning is a simple Reinforcement Learning algorithm which is able to provide the agent with some memory. Q-learning allows the agent to use the environment's rewards to learn, which over time will provide it with the best action to take when in a given state. \n",
    "\n",
    "Specifically, for the Taxi environment, the agent will learn from the rewards table `P`. It does this by looking at what rewards will be given for taking an action in a current state, and then updating a Q-value to remember if the action was beneficial in the long run.\n",
    "\n",
    "The Q-values are stored in a table called Q-table, where the rows represent the states, and the columns represent the actions possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the Q-table is initialised, with the size being `(number of possible states, number of actions possible)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the training algorithm for Q-learning is implemented, it will update the Q-table as the agent explores the environment over many episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "graph = False\n",
    "num_episodes = 100001\n",
    "\n",
    "#graph for comparision with Cart Pole \n",
    "if graph == True:\n",
    "    num_episodes = 500 #500 for graphing, 100001 for normal\n",
    "\n",
    "#store how many timesteps per episode\n",
    "steps = np.zeros(num_episodes)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, num_episodes):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if graph == True:\n",
    "            steps[i] += 1\n",
    "        #decide whether to pick a random action or use already computed Q-values. \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "        \n",
    "        #execute the chosen action\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        #store value for previous state, and for current state (after executing action)\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        #calculate the maximum Q-value for the current state (after executing action)\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        #update Q-table\n",
    "        q_table[state, action] = new_value\n",
    "        \n",
    "        #penalise the agent if agents drops off / picks up\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        \n",
    "        #set the next state and increment the timestep\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "    \n",
    "    #print out how many episodes have been run\n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "if graph == True:\n",
    "    sns.lineplot(range(len(steps)),steps)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Steps\")\n",
    "    plt.title(\"Taxi-Problem\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-table has been establised over 100,000 episodes. The Q-values for state 328 can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now simply evaluate the performance of the agent after 'learning'. There is no need to explore any actions anymore as we can be sure the agents has learned which action to select next using the best Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        #get the best action to take, by finding the maximum Q-value for a given state.\n",
    "        action = np.argmax(q_table[state])\n",
    "        #execute the action\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        #if agent runs into a wall, give it a penalty\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        \n",
    "        #increment timestep\n",
    "        epochs += 1\n",
    "    \n",
    "    #update total penalties accumilated, and timesteps\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
