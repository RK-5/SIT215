{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tutorial has been used to implement Q-Learning:\n",
    "https://www.kaggle.com/sarjit07/reinforcement-learning-using-q-table-frozenlake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using non slippery frozen lake, modify registers\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='FrozenLakeNotSlippery-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '4x4', 'is_slippery': False},\n",
    ")\n",
    "\n",
    "#Load the game environment and render what it looks like\n",
    "env = gym.make('FrozenLakeNotSlippery-v0')\n",
    "\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of States and Actions\n",
    "number_of_states = env.observation_space.n\n",
    "number_of_actions = env.action_space.n\n",
    "\n",
    "print( \"States = \", number_of_states)\n",
    "print( \"Actions = \", number_of_actions)\n",
    "\n",
    "num_episodes = 1000\n",
    "steps_total = []\n",
    "rewards_total = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if the gamma/ discount rate is 0, agent will only account current reward\n",
    "#if the gamma/ discount rate is 1, agent will account future rewards too\n",
    "gamma = 0.95\n",
    "\n",
    "#if the learning rate is 0, agent will pick next action based on past learning.\n",
    "#if the learning rate is 1, agent will pick the next action based on current situation.\n",
    "learning_rate = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the agent to initially start with 70% of actions being random, to explore the environment.\n",
    "# Adjust epison as more actions are learned from past experience.\n",
    "egreedy = 0.7\n",
    "egreedy_final = 0.1\n",
    "egreedy_decay = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise Q-Table with 0s, and print it. Rows represent the states, and columns represent the actions.\n",
    "Q = torch.zeros([number_of_states, number_of_actions])\n",
    "Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training agent\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    # resets the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        #increment the timestep\n",
    "        step += 1\n",
    "        \n",
    "        random_for_egreedy = torch.rand(1)[0]\n",
    "        \n",
    "        #decide whether to pick a random action or use already computed Q-values.\n",
    "        if random_for_egreedy > egreedy:      \n",
    "            random_values = Q[state] + torch.rand(1,number_of_actions) / 1000      \n",
    "            action = torch.max(random_values,1)[1][0]  \n",
    "            action = action.item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        if egreedy > egreedy_final:\n",
    "            egreedy *= egreedy_decay\n",
    "        \n",
    "        #execute the chosen action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # calculate and update q-value for current state and action\n",
    "        Q[state, action] = reward + gamma * torch.max(Q[new_state])\n",
    "        \n",
    "        #set the next state\n",
    "        state = new_state\n",
    "        \n",
    "        # env.render()\n",
    "        # time.sleep(0.4)\n",
    "        \n",
    "        if done:\n",
    "            #update total timesteps, \n",
    "            steps_total.append(step)\n",
    "            rewards_total.append(reward)\n",
    "            \n",
    "            if i_episode % 10 == 0:\n",
    "                clear_output(wait=True)\n",
    "                print('Episode: {} Reward: {} Steps Taken: {}'.format(i_episode,reward, step))\n",
    "            break\n",
    "            \n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print final Q-table\n",
    "print(Q)\n",
    "\n",
    "print(\"Episodes finished successfully: {0}\".format(sum(rewards_total)))\n",
    "print(\"Percent of episodes finished successfully: {0}\".format(sum(rewards_total)/num_episodes)*100)\n",
    "\n",
    "print(\"Average number of steps: %.2f\" % (sum(steps_total)/num_episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate plot\n",
    "sns.lineplot(range(len(steps_total)),steps_total)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.title(\"FrozenLake-Problem\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
